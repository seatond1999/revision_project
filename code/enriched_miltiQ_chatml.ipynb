{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an AI examiner who will ask concise questions about information which will be provided.<|im_end|>\n",
      "<|im_start|>user\n",
      "Information: ###The optimal age at which to perform orchiopexy for cryptorchidism has long been debated. The aim of this study was to determine if age at orchiopexy affected testicular atrophy.A retrospective review of patients undergoing orchiopexy from 2000 to 2010 was conducted. An individual testis, rather than patient, was used as the dependent variable. A total of 349 testicles from 1126 charts (ICD-9=752.51) were identified. Primary study outcome was testicular survival without atrophy.Mean follow up for the study was 25 months. There was postoperative atrophy in 27 testes (7.7%). Intraabdominal testicle was independently associated with increased postsurgical atrophy (p<0.0001). The odds of postsurgical atrophy were 15.66 times higher for an abdominal vs. inguinal location (95% CI: 5.5-44.6). Testicular atrophy was highest for orchiopexy at ages 13-24 months (n=16 of 133, 12%) vs. those less than 13 months (n=3 of 64, 5%), and those greater than 24 months (n=8 of 152, 5%) (p=0.0024). After adjusting for location, age was not statistically significant with postsurgical atrophy (p=0.055).\n",
      "Obstructive sleep apnea (OSA) is tightly linked to increased cardiovascular disease. Surgery is an important method to treat OSA, but its effect on serum lipid levels in OSA patients is unknown. We aimed to evaluate the effect of upper airway surgery on lipid profiles.We performed a retrospective review of 113 adult patients with OSA who underwent surgery (nasal or uvulopalatopharyngoplasty [UPPP]) at a major, urban, academic hospital in Beijing from 2012 to 2013 who had preoperative and postoperative serum lipid profiles.Serum TC (4.86±0.74 to 4.69±0.71) and LP(a) (median 18.50 to 10.90) all decreased significantly post-operatively (P<0.01, 0.01, respectively), with no changes in serum HDL, LDL, or TG (P>0.05, all). For UPPP patients (n=51), serum TC, HDL and LP(a) improved (P=0.01, 0.01,<0.01, respectively). For nasal patients (n=62), only the serum LP(a) decreased (P<0.01). In patients with normal serum lipids at baseline, only serum LP(a) decreased (P<0.01). In contrast, in patients with isolated hypertriglyceridemia, the serum HDL, TG and LP(a) showed significant improvements (P=0.02, 0.03,<0.01, respectively). In patients with isolated hypercholesterolemia, the serum LP(a) decreased significantly (P=0.01), with a similar trend for serum TC (P=0.06). In patients with mixed hyperlipidemia, the serum TC and LDL also decreased (P=0.02, 0.03, respectively).###\n",
      "Ask me questions about this information.<|im_end|>\n",
      "<|im_start|>assisstant\n",
      "Undescended testes: does age at orchiopexy affect survival of the testis?\n",
      "What was the primary study outcome in this retrospective review of patients undergoing orchiopexy for cryptorchidism?\n",
      "Does airway surgery lower serum lipid levels in obstructive sleep apnea patients?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(prep_data()['train']['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\DanSeaton\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GPTQConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import tempfile\n",
    "from huggingface_hub import login\n",
    "from datetime import datetime as dt\n",
    "import torch\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "## timer\n",
    "def timer(func):\n",
    "    def do(x):\n",
    "        before = dt.now()\n",
    "        perform = func(x)\n",
    "        after = dt.now()\n",
    "        print(after-before)\n",
    "        return perform\n",
    "    return do\n",
    "\n",
    "# %%\n",
    "token = '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'\n",
    "login(token = token)\n",
    "\n",
    "# preprocessing:\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    data = pd.read_csv(r'../ready_data.csv')\n",
    "    # context had disctionary, only interested in the 'contexts' key\n",
    "    system = \"You are an AI examiner who will ask concise questions about information which will be provided.\"\n",
    "    # create data for asking Qs (aq)\n",
    "    data_aq = pd.DataFrame(\n",
    "        {\n",
    "            \"content\": data.apply(\n",
    "                lambda x: \"<|im_start|>system\" + f'\\n{system}<|im_end|>'\n",
    "                + \"\\n<|im_start|>user\" + f\"\\nInformation: ###{x['full_context']}###\\nAsk me questions about this information.<|im_end|>\"\n",
    "                + \"\\n<|im_start|>assisstant\" + f\"\\n{x['full_questions']}<|im_end|>\",\n",
    "                axis=1,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    data_aq = Dataset.from_pandas(data_aq)\n",
    "    data_aq = data_aq.train_test_split(test_size=0.016)\n",
    "    return data_aq\n",
    "\n",
    "@timer\n",
    "def finetune(data):\n",
    "    train_data = data[\"train\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    ##load model and tokenizer\n",
    "    model_id = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=False)\n",
    "    \n",
    "\n",
    "    quantization_config_loading = GPTQConfig(\n",
    "        bits=4, disable_exllama=True, tokenizer=tokenizer\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=quantization_config_loading, device_map=\"auto\"\n",
    "    )\n",
    "    # add special tokens for chatML format ##specific to this experiment:\n",
    "    tokenizer.pad_token = \"</s>\"\n",
    "    tokenizer.add_tokens([\"<|im_start|>\"])\n",
    "    tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    ##finetune:\n",
    "\n",
    "    print(model)\n",
    "    model.config.use_cache = False  # wont store intermediate states\n",
    "    model.config.pretraining_tp = 1  # to replicate pre-training performance\n",
    "    model.gradient_checkpointing_enable()  # compramise between forgetting activation states and remmebering them for backpropagation. Trade off computation time for GPU memory\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        modules_to_save = [\"lm_head\", \"embed_tokens\"] #because of special tokens #### sepcific to this experiemtn\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    ##\n",
    "    name = \"chatml_finetune_mistral_aq\"\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=name,\n",
    "        per_device_train_batch_size=8, #5 works\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",  # so do we need the whole PeftSavingCallback function? maybe try withput and run the trainer(from last check=true)\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        #max_steps=250,\n",
    "        fp16=True,\n",
    "        push_to_hub=True,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        ddp_find_unused_parameters=False\n",
    "    )\n",
    "\n",
    "    # if checkpint doesnt work, might have to do save_steps=20 for example in the training argumnet above\n",
    "\n",
    "    ############## checkpoint ##############################\n",
    "    class PeftSavingCallback(TrainerCallback):\n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            checkpoint_path = os.path.join(\n",
    "                args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "            )\n",
    "            kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "            if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "                os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"content\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=callbacks, #try if doesnt work hashing all of checkpiint stuff above and also this callback line\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    ########## set up tensorboard #####################\n",
    "    #tmpdir = tempfile.TemporaryDirectory()\n",
    "    #local_training_root = tmpdir.name\n",
    "\n",
    "    #loc_checkpoint_path = os.path.join(local_training_root, name)\n",
    "    #tensorboard_display_dir = f\"{loc_checkpoint_path}/runs\"\n",
    "\n",
    "    #%load_ext tensorboard\n",
    "    #%tensorboard --logdir f'{tensorboard_display_dir}'\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "    #trainer.train(resume_from_checkpoint=True) #use if want to go from checkpoint\n",
    "    trainer.train()\n",
    "    #trainer.state.log_history()\n",
    "    #trainer.save_model()\n",
    "    #trainer.push_to_hub() #un hash when want to send final model to hub\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# resume_from_checkpoint (str or bool, optional) — If a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "          (k_proj): QuantLinear()\n",
      "          (o_proj): QuantLinear()\n",
      "          (q_proj): QuantLinear()\n",
      "          (v_proj): QuantLinear()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (act_fn): SiLU()\n",
      "          (down_proj): QuantLinear()\n",
      "          (gate_proj): QuantLinear()\n",
      "          (up_proj): QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55a2303790840bf951a312ae5bfd61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1225fe25e7914a3297c9edddecfa5bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/seatond/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 22:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:22:45.708326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% --------------------------------------------------------------------------\n",
    "# RUN!\n",
    "if __name__ == '__main__':\n",
    "    trainer_obj = finetune(prep_data())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-655dd584-47d863f221d573292a80c61f;f7283fa1-6072-485a-ad03-8046c8021c0e)\n\nRepository Not Found for url: https://huggingface.co/api/models/mist_question_asking.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2546\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   2545\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2546\u001b[0m     hf_raise_for_status(r)\n\u001b[1;32m   2547\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:303\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create (Request ID: Root=1-655dd584-6019da280189f1c901a5fbdf;b43b0f01-9bfe-4a15-976c-c4861ac73e95)\n\nYou don't have the rights to create a model under this namespace",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/models/mist_question_asking",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_obj\u001b[39m.\u001b[39;49mpush_to_hub()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3686\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3684\u001b[0m \u001b[39m# In case the user calls this method with args.push_to_hub = False\u001b[39;00m\n\u001b[1;32m   3685\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3686\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_hf_repo()\n\u001b[1;32m   3688\u001b[0m \u001b[39m# Needs to be executed on all processes for TPU training, but will only save on the processed determined by\u001b[39;00m\n\u001b[1;32m   3689\u001b[0m \u001b[39m# self.args.should_save.\u001b[39;00m\n\u001b[1;32m   3690\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_model(_internal_call\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3480\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     repo_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id\n\u001b[0;32m-> 3480\u001b[0m repo_url \u001b[39m=\u001b[39m create_repo(repo_name, token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_token, private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m repo_url\u001b[39m.\u001b[39mrepo_id\n\u001b[1;32m   3482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush_in_progress \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2554\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[39melif\u001b[39;00m exist_ok \u001b[39mand\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m403\u001b[39m:\n\u001b[1;32m   2552\u001b[0m     \u001b[39m# No write permission on the namespace but repo might already exist\u001b[39;00m\n\u001b[1;32m   2553\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2554\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepo_info(repo_id\u001b[39m=\u001b[39;49mrepo_id, repo_type\u001b[39m=\u001b[39;49mrepo_type, token\u001b[39m=\u001b[39;49mtoken)\n\u001b[1;32m   2555\u001b[0m         \u001b[39mif\u001b[39;00m repo_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m repo_type \u001b[39m==\u001b[39m REPO_TYPE_MODEL:\n\u001b[1;32m   2556\u001b[0m             \u001b[39mreturn\u001b[39;00m RepoUrl(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1888\u001b[0m, in \u001b[0;36mHfApi.repo_info\u001b[0;34m(self, repo_id, revision, repo_type, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   1886\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnsupported repo type.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1888\u001b[0m \u001b[39mreturn\u001b[39;00m method(\n\u001b[1;32m   1889\u001b[0m     repo_id,\n\u001b[1;32m   1890\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1891\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1892\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1893\u001b[0m     files_metadata\u001b[39m=\u001b[39;49mfiles_metadata,\n\u001b[1;32m   1894\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1698\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[0;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, token)\u001b[0m\n\u001b[1;32m   1696\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mblobs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m r \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mget(path, headers\u001b[39m=\u001b[39mheaders, timeout\u001b[39m=\u001b[39mtimeout, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m-> 1698\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1699\u001b[0m d \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n\u001b[1;32m   1700\u001b[0m \u001b[39mreturn\u001b[39;00m ModelInfo(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39md)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepoNotFound\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m:\n\u001b[1;32m    280\u001b[0m     \u001b[39m# 401 is misleading as it is returned for:\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[39m#    - private and gated repos if user is not authenticated\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[39m#    - missing repos\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[39m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[39m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n\u001b[1;32m    296\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    297\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request for \u001b[39m\u001b[39m{\u001b[39;00mendpoint_name\u001b[39m}\u001b[39;00m\u001b[39m endpoint:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m endpoint_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mBad request:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     )\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-655dd584-47d863f221d573292a80c61f;f7283fa1-6072-485a-ad03-8046c8021c0e)\n\nRepository Not Found for url: https://huggingface.co/api/models/mist_question_asking.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated."
     ]
    }
   ],
   "source": [
    "trainer_obj.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
