{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/seatond/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GPTQConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import tempfile\n",
    "from huggingface_hub import login\n",
    "from datetime import datetime as dt\n",
    "import torch\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "## timer\n",
    "def timer(func):\n",
    "    def do(x):\n",
    "        before = dt.now()\n",
    "        perform = func(x)\n",
    "        after = dt.now()\n",
    "        print(after-before)\n",
    "        return perform\n",
    "    return do\n",
    "\n",
    "# %%\n",
    "token = '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'\n",
    "login(token = token)\n",
    "\n",
    "# preprocessing:\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    data = pd.read_csv(r'../ready_data.csv')\n",
    "    # context had disctionary, only interested in the 'contexts' key\n",
    "\n",
    "    # create data for asking Qs (aq)\n",
    "    data_aq = pd.DataFrame(\n",
    "        {\n",
    "            \"content\": data.apply(\n",
    "                lambda x: \"###human: Ask me a questions about:\\n\"\n",
    "                + x[\"full_context\"]\n",
    "                + \"\\n###Response:\\n\"\n",
    "                + x[\"full_questions\"]\n",
    "                +\" 2\" +\",\n",
    "                axis=1,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    data_aq = Dataset.from_pandas(data_aq)\n",
    "    data_aq = data_aq.train_test_split(test_size=0.016)\n",
    "    return data_aq\n",
    "\n",
    "@timer\n",
    "def finetune(data):\n",
    "    train_data = data[\"train\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    ##load model and tokenizer\n",
    "    model_id = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(tokenizer.eos_token + 'HUISDHFIUEHUIFEHIUEUIFEHW')\n",
    "    quantization_config_loading = GPTQConfig(\n",
    "        bits=4, disable_exllama=True, tokenizer=tokenizer\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=quantization_config_loading, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    ##finetune:\n",
    "\n",
    "    print(model)\n",
    "    model.config.use_cache = False  # wont store intermediate states\n",
    "    model.config.pretraining_tp = 1  # to replicate pre-training performance\n",
    "    model.gradient_checkpointing_enable()  # compramise between forgetting activation states and remmebering them for backpropagation. Trade off computation time for GPU memory\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    name = \"ADDING A 2\"\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=name,\n",
    "        per_device_train_batch_size=8, #5 works\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",  # so do we need the whole PeftSavingCallback function? maybe try withput and run the trainer(from last check=true)\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        #max_steps=250,\n",
    "        fp16=True,\n",
    "        #push_to_hub=True,\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    # if checkpint doesnt work, might have to do save_steps=20 for example in the training argumnet above\n",
    "\n",
    "    ############## checkpoint ##############################\n",
    "    class PeftSavingCallback(TrainerCallback):\n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            checkpoint_path = os.path.join(\n",
    "                args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "            )\n",
    "            kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "            if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "                os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"content\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=callbacks, #try if doesnt work hashing all of checkpiint stuff above and also this callback line\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    ########## set up tensorboard #####################\n",
    "    #tmpdir = tempfile.TemporaryDirectory()\n",
    "    #local_training_root = tmpdir.name\n",
    "\n",
    "    #loc_checkpoint_path = os.path.join(local_training_root, name)\n",
    "    #tensorboard_display_dir = f\"{loc_checkpoint_path}/runs\"\n",
    "\n",
    "    #%load_ext tensorboard\n",
    "    #%tensorboard --logdir f'{tensorboard_display_dir}'\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "    #trainer.train(resume_from_checkpoint=True) #use if want to go from checkpoint\n",
    "    trainer.train()\n",
    "    #trainer.state.log_history()\n",
    "    #trainer.save_model()\n",
    "    #trainer.push_to_hub() #un hash when want to send final model to hub\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# resume_from_checkpoint (str or bool, optional) â€” If a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>HUISDHFIUEHUIFEHIUEUIFEHW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "          (k_proj): QuantLinear()\n",
      "          (o_proj): QuantLinear()\n",
      "          (q_proj): QuantLinear()\n",
      "          (v_proj): QuantLinear()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (act_fn): SiLU()\n",
      "          (down_proj): QuantLinear()\n",
      "          (gate_proj): QuantLinear()\n",
      "          (up_proj): QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60822acaa64429280deeb244fcbff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839721e38a2244728993a2d1392fefb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/seatond/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 23:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:24:07.256782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% --------------------------------------------------------------------------\n",
    "# RUN!\n",
    "if __name__ == '__main__':\n",
    "    trainer_obj = finetune(prep_data())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "model_id = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab574daa09b348bca8c11cc9159dca6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/965 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941adbcf698f409c8609fdf067b03bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e0015ad73944748623960a9e2ee6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89867f1ef42a4110955c89ad015514e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d82e47645c4ca1944be31bb1d7954a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_config.json:   0%|          | 0.00/497 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc86d7d14ab42938b2e413ca9667d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)er_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "#for inference\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, GenerationConfig, GPTQConfig\n",
    "import torch\n",
    "\n",
    "#model_id_inf = \"seatond/mist_question_asking_5e4LR_2epoch\" #2 epoch trained model on non enriched, non multi Q data\n",
    "model_id_inf = \"seatond/multiqaskingmistral\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_inf, use_fast=True)\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = model_id_inf,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:0\",\n",
    "    token='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'\n",
    "    )\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###human: Ask me question about:\n",
      "('Halofantrine is a newly developed antimalarial drug used for the treatment of Plasmodium falciparum malaria. The introduction of this drug has been delayed because of its possible side effects, and due to insufficient studies on adverse reactions in humans. There have been no studies investigating its effect on hearing.', 'Thirty guinea pigs were divided into three groups: a control group, a halofantrine therapeutic dose group and a halofantrine double therapeutic dose group. One cochlea specimen from each animal was stained with haematoxylin and eosin and the other with toluidine blue.', 'No changes were detected in the control group. The halofantrine therapeutic dose group showed loss and distortion of inner hair cells and inner phalangeal cells, and loss of spiral ganglia cells. In the halofantrine double therapeutic dose group, the inner and outer hair cells were distorted and there was loss of spiral ganglia cells.')\n",
      "###Response:\n",
      "Is halofantrine toxic to the cochlea?\n",
      "What were the three groups of guinea pigs in the study?\n",
      "What changes were observed in the cochlea of the halofantrine therapeutic dose group?\n",
      "What was the effect of halofantrine on the inner and outer hair cells in the halofantrine double therapeutic dose group?\n",
      "What staining method was used to examine the cochlea specimens in the study?\n",
      "What were the two groups of guinea pigs that received halofantrine in the study?\n",
      "What were the changes observed in the cochlea of the halofantrine therapeutic dose group?\n",
      "What was the effect of halofantrine on the spiral ganglia cells in the halofantrine double therapeutic dose group?\n",
      "What was the purpose of the study on halofantrine and its effect on the cochlea?\n",
      "What was the staining method used to examine the cochlea specimens in the study?\n",
      "\n",
      "##Response:\n",
      "Is halofantrine toxic to the cochlea?\n",
      "What changes were observed in the cochlea of the halofantrine therapeutic dose group?\n",
      "What was the effect of halofantrine on the inner and outer hair cells in the halofantrine double therapeutic dose group?\n",
      "What staining method was used to examine the cochlea specimens in the study?\n",
      "What were the two groups of guinea pigs that received halofantrine in the study?\n",
      "What were the changes observed in the cochlea of the halofantrine therapeutic dose group?\n",
      "What was the purpose of the study on halofantrine and its effect on the cochlea?\n",
      "What was the effect of halofantrine on the spiral ganglia cells in the halofantrine double therapeutic dose group?\n",
      "What was the staining method used to examine the cochlea specimens in the study?\n",
      "What were the groups of guinea pigs in the study?\n",
      "\n",
      "To what extent does halofantrine affect the cochlea?\n",
      "What changes were observed in the cochlea of the halof\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "context = \"Halofantrine is a newly developed antimalarial drug used for the treatment of Plasmodium falciparum malaria. The introduction of this drug has been delayed because of its possible side effects, and due to insufficient studies on adverse reactions in humans. There have been no studies investigating its effect on hearing.\", \"Thirty guinea pigs were divided into three groups: a control group, a halofantrine therapeutic dose group and a halofantrine double therapeutic dose group. One cochlea specimen from each animal was stained with haematoxylin and eosin and the other with toluidine blue.\", \"No changes were detected in the control group. The halofantrine therapeutic dose group showed loss and distortion of inner hair cells and inner phalangeal cells, and loss of spiral ganglia cells. In the halofantrine double therapeutic dose group, the inner and outer hair cells were distorted and there was loss of spiral ganglia cells.\"\n",
    "prompt_template = f'###human: Ask me question about:\\n{context}\\n###Response:\\n'\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.to('cuda:0')\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    top_k=1,\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=30,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "outputs = model.generate(inputs=input_ids, generation_config=generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)\n",
    "print(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
