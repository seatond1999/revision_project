{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/seatond/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# %% --------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GPTQConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import tempfile\n",
    "from huggingface_hub import login\n",
    "from datetime import datetime as dt\n",
    "import torch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## timer\n",
    "def timer(func):\n",
    "    def do(x):\n",
    "        before = dt.now()\n",
    "        perform = func(x)\n",
    "        after = dt.now()\n",
    "        print(after - before)\n",
    "        return perform\n",
    "\n",
    "    return do\n",
    "\n",
    "\n",
    "# %%\n",
    "token = \"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\"\n",
    "login(token=token)\n",
    "\n",
    "# preprocessing:\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    data = (pd.read_csv(r'../enriched_data.csv'))\n",
    "    data[\"combined_questions\"] = data[\"question\"] + \"\\n\" + data[\"gpt_question\"]\n",
    "    data.rename(columns={'context':'full_context','combined_questions':'full_questions'},inplace=True)\n",
    "    data = data.iloc[0:504,]\n",
    "\n",
    "    # context had disctionary, only interested in the 'contexts' key\n",
    "\n",
    "    # create data for asking Qs (aq)\n",
    "    data_aq = pd.DataFrame(\n",
    "        {\n",
    "            \"content\": data.apply(\n",
    "                lambda x: \"###human: Ask me a questions about:\\n\"\n",
    "                + x[\"full_context\"]\n",
    "                + \"\\n###Response:\\n\"\n",
    "                + x[\"full_questions\"],\n",
    "                axis=1,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    data_aq = Dataset.from_pandas(data_aq)\n",
    "    data_aq = data_aq.train_test_split(test_size=0.016)\n",
    "    return data_aq\n",
    "\n",
    "\n",
    "@timer\n",
    "def finetune(data):\n",
    "    train_data = data[\"train\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    ##load model and tokenizer\n",
    "    model_id = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    quantization_config_loading = GPTQConfig(\n",
    "        bits=4, disable_exllama=True, tokenizer=tokenizer\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=quantization_config_loading, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    ##finetune:\n",
    "\n",
    "    print(model)\n",
    "    model.config.use_cache = False  # wont store intermediate states\n",
    "    model.config.pretraining_tp = 1  # to replicate pre-training performance\n",
    "    model.gradient_checkpointing_enable()  # compramise between forgetting activation states and remmebering them for backpropagation. Trade off computation time for GPU memory\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    name = \"enrich_ONLYYY\"\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=name,\n",
    "        per_device_train_batch_size=8,  # 5 works\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",  # so do we need the whole PeftSavingCallback function? maybe try withput and run the trainer(from last check=true)\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        # max_steps=250,\n",
    "        fp16=True,\n",
    "        #push_to_hub=True,\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    # if checkpint doesnt work, might have to do save_steps=20 for example in the training argumnet above\n",
    "\n",
    "    ############## checkpoint ##############################\n",
    "    class PeftSavingCallback(TrainerCallback):\n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            checkpoint_path = os.path.join(\n",
    "                args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "            )\n",
    "            kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "            if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "                os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"content\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=callbacks,  # try if doesnt work hashing all of checkpiint stuff above and also this callback line\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    ########## set up tensorboard #####################\n",
    "    # tmpdir = tempfile.TemporaryDirectory()\n",
    "    # local_training_root = tmpdir.name\n",
    "\n",
    "    # loc_checkpoint_path = os.path.join(local_training_root, name)\n",
    "    # tensorboard_display_dir = f\"{loc_checkpoint_path}/runs\"\n",
    "\n",
    "    # %load_ext tensorboard\n",
    "    # %tensorboard --logdir f'{tensorboard_display_dir}'\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "    # trainer.train(resume_from_checkpoint=True) #use if want to go from checkpoint\n",
    "    trainer.train()\n",
    "    # trainer.state.log_history()\n",
    "    # trainer.save_model()\n",
    "    # trainer.push_to_hub() #un hash when want to send final model to hub\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# resume_from_checkpoint (str or bool, optional) â€” If a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. use_exllama, exllama_config, use_cuda_fp16, max_input_length) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "          (k_proj): QuantLinear()\n",
      "          (o_proj): QuantLinear()\n",
      "          (q_proj): QuantLinear()\n",
      "          (v_proj): QuantLinear()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (act_fn): SiLU()\n",
      "          (down_proj): QuantLinear()\n",
      "          (gate_proj): QuantLinear()\n",
      "          (up_proj): QuantLinear()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7128e88e0b4f758b8ca9e1f7a11431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1e3648a12448e8bef787c18bb047d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seatond/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/seatond/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 14:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:14:30.313285\n"
     ]
    }
   ],
   "source": [
    "# %% --------------------------------------------------------------------------\n",
    "# RUN!\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_obj = finetune(prep_data())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
