{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# %% --------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GPTQConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import tempfile\n",
    "from huggingface_hub import login\n",
    "from datetime import datetime as dt\n",
    "import torch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "## timer\n",
    "def timer(func):\n",
    "    def do(x):\n",
    "        before = dt.now()\n",
    "        perform = func(x)\n",
    "        after = dt.now()\n",
    "        print(after - before)\n",
    "        return perform\n",
    "\n",
    "    return do\n",
    "\n",
    "\n",
    "# %%\n",
    "token = \"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\"\n",
    "login(token=token)\n",
    "\n",
    "# preprocessing:\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    data = (pd.read_csv(r'../enriched_data.csv'))\n",
    "    data[\"combined_questions\"] = data[\"question\"] + \"\\n\" + data[\"gpt_question\"]\n",
    "    data.rename(columns={'context':'full_context','combined_questions':'full_questions'},inplace=True)\n",
    "    data = data.iloc[0:504,]\n",
    "\n",
    "    # context had disctionary, only interested in the 'contexts' key\n",
    "\n",
    "    # create data for asking Qs (aq)\n",
    "    data_aq = pd.DataFrame(\n",
    "        {\n",
    "            \"content\": data.apply(\n",
    "                lambda x: \"###human: Ask me a questions about:\\n\"\n",
    "                + x[\"full_context\"]\n",
    "                + \"\\n###Response:\\n\"\n",
    "                + x[\"full_questions\"],\n",
    "                axis=1,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    data_aq = Dataset.from_pandas(data_aq)\n",
    "    data_aq = data_aq.train_test_split(test_size=0.016)\n",
    "    return data_aq\n",
    "\n",
    "\n",
    "@timer\n",
    "def finetune(data):\n",
    "    train_data = data[\"train\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    ##load model and tokenizer\n",
    "    model_id = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    quantization_config_loading = GPTQConfig(\n",
    "        bits=4, disable_exllama=True, tokenizer=tokenizer\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=quantization_config_loading, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    ##finetune:\n",
    "\n",
    "    print(model)\n",
    "    model.config.use_cache = False  # wont store intermediate states\n",
    "    model.config.pretraining_tp = 1  # to replicate pre-training performance\n",
    "    model.gradient_checkpointing_enable()  # compramise between forgetting activation states and remmebering them for backpropagation. Trade off computation time for GPU memory\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    name = \"enrich_ONLYYY\"\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=name,\n",
    "        per_device_train_batch_size=8,  # 5 works\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",  # so do we need the whole PeftSavingCallback function? maybe try withput and run the trainer(from last check=true)\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        # max_steps=250,\n",
    "        fp16=True,\n",
    "        #push_to_hub=True,\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "\n",
    "    # if checkpint doesnt work, might have to do save_steps=20 for example in the training argumnet above\n",
    "\n",
    "    ############## checkpoint ##############################\n",
    "    class PeftSavingCallback(TrainerCallback):\n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            checkpoint_path = os.path.join(\n",
    "                args.output_dir, f\"checkpoint-{state.global_step}\"\n",
    "            )\n",
    "            kwargs[\"model\"].save_pretrained(checkpoint_path)\n",
    "\n",
    "            if \"pytorch_model.bin\" in os.listdir(checkpoint_path):\n",
    "                os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n",
    "\n",
    "    callbacks = [PeftSavingCallback()]\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"content\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=callbacks,  # try if doesnt work hashing all of checkpiint stuff above and also this callback line\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    ########## set up tensorboard #####################\n",
    "    # tmpdir = tempfile.TemporaryDirectory()\n",
    "    # local_training_root = tmpdir.name\n",
    "\n",
    "    # loc_checkpoint_path = os.path.join(local_training_root, name)\n",
    "    # tensorboard_display_dir = f\"{loc_checkpoint_path}/runs\"\n",
    "\n",
    "    # %load_ext tensorboard\n",
    "    # %tensorboard --logdir f'{tensorboard_display_dir}'\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "    # trainer.train(resume_from_checkpoint=True) #use if want to go from checkpoint\n",
    "    trainer.train()\n",
    "    # trainer.state.log_history()\n",
    "    # trainer.save_model()\n",
    "    # trainer.push_to_hub() #un hash when want to send final model to hub\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# resume_from_checkpoint (str or bool, optional) â€” If a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a bool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance of Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --------------------------------------------------------------------------\n",
    "# RUN!\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_obj = finetune(prep_data())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
