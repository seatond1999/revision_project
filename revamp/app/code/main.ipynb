{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KHqpZelimvN",
        "outputId": "cbdb06b8-8b13-4408-b36d-6c61187e4bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.25.2)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.42.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Collecting auto-gptq\n",
            "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.0%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.28.0.dev0)\n",
            "Collecting datasets (from auto-gptq)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.25.2)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.39.0.dev0)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto-gptq)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Collecting multiprocess (from datasets->auto-gptq)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Installing collected packages: rouge, gekko, dill, multiprocess, datasets, auto-gptq\n",
            "Successfully installed auto-gptq-0.7.0+cu118 datasets-2.17.1 dill-0.3.8 gekko-1.0.6 multiprocess-0.70.16 rouge-1.0.1\n",
            "Collecting optimum\n",
            "  Downloading optimum-1.17.1-py3-none-any.whl (407 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from optimum)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.39.0.dev0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.1.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.25.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.20.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum) (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.4.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum) (3.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum) (3.9.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum) (1.16.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, optimum\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.17.1\n",
            "Collecting trl\n",
            "  Downloading trl-0.7.11-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.39.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.25.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.28.0.dev0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.17.1)\n",
            "Collecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.7.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (4.66.2)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.7.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n",
            "Installing collected packages: shtab, docstring-parser, tyro, trl\n",
            "Successfully installed docstring-parser-0.15 shtab-1.7.0 trl-0.7.11 tyro-0.7.3\n",
            "Collecting py7zr\n",
            "  Downloading py7zr-0.20.8-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable (from py7zr)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.15.9 (from py7zr)\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.1.0 (from py7zr)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
            "Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.20.8 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.9 texttable-1.7.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWqzIVl8Q2lk",
        "outputId": "48d0dc45-f67c-4af4-f8af-c452a9c1f8a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/app/code\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    GPTQConfig,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "import torch\n",
        "from torch import nn\n",
        "from openai import OpenAI\n",
        "import openai\n",
        "import os\n",
        "import time\n",
        "from PyPDF2 import PdfReader\n",
        "import tensorboard\n",
        "from auto_gptq import exllama_set_max_input_length\n",
        "\n",
        "\n",
        "#to get into app folder...\n",
        "\n",
        "%cd drive/MyDrive/app/code\n",
        "\n",
        "\n",
        "#from app.code.help import load_base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pWjglbuRePoj",
        "outputId": "dd61dcd1-f2ca-46ee-fae8-48977055f35b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/app/code'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAiqRWQOKJFv"
      },
      "outputs": [],
      "source": [
        "# %% --------------------------------------------------------------------------\n",
        "\n",
        "#remember for this script need to havemodified mistral_modelling.py file\n",
        "\n",
        "\n",
        "def load_base_model(base_model):\n",
        "    base_path = base_model  # input: base model\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
        "    tokenizer.pad_token = \"<unk>\"\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    quantization_config_loading = GPTQConfig(\n",
        "        bits=4,\n",
        "        #disable_exllama=True, #because led to inrease in inference time by 10 times..\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_path,\n",
        "        return_dict=True,\n",
        "        quantization_config=quantization_config_loading,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    #model = exllama_set_max_input_length(model, max_input_length=33400)\n",
        "\n",
        "    tokenizer.pad_token = \"<unk>\"\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    model.pad_token_id = tokenizer.pad_token_id\n",
        "    model.eos_token_id = tokenizer.eos_token_id #these 2 lines are diff to usual?\n",
        "\n",
        "    #model.config.use_return_dict = True #so changes to source code work.\n",
        "\n",
        "    return tokenizer,model\n",
        "\n",
        "class app:\n",
        "    def __init__(self,base_model_path,book):\n",
        "        self.book = book\n",
        "        self.base_model_path = base_model_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.fpclass_head = None\n",
        "        self.contclass_head = None\n",
        "        self.causal_head = None\n",
        "        self.page_lag = None\n",
        "        self.contents_first = None\n",
        "        self.contents_last = None\n",
        "        self.contents = None #make pd dataframe but get last contents page first lol.\n",
        "        self.chapter_breakdown = None\n",
        "\n",
        "\n",
        "    def get_base(self):\n",
        "        self.tokenizer, self.model = load_base_model(self.base_model_path)\n",
        "        self.causal_head = self.model.lm_head #no finetuning was done to causal head, unlike classificaiton head, so can take the base model LM_head for base and causal.\n",
        "\n",
        "    def load_fpclass_adapter(self):\n",
        "        self.model.num_labels = 2\n",
        "        self.model.config.num_labels = 2 #model attributes needed for classificaiton forward pass\n",
        "        self.model.score = nn.Linear(self.model.config.hidden_size, self.model.config.num_labels, bias=False,dtype=torch.float16) #bias is false as finetuned with and without and made no diff so saves memory\n",
        "        self.model.load_adapter(peft_model_id=r'../adapters/firstpage',adapter_name='ident_fistpage_adapter')\n",
        "        self.fpclass_head = self.model.score\n",
        "        return None\n",
        "\n",
        "    def load_contclass_adapter(self):\n",
        "        self.model.num_labels = 2\n",
        "        self.model.config.num_labels = 2 #model attributes needed for classificaiton forward pass\n",
        "        self.model.score = nn.Linear(self.model.config.hidden_size, self.model.config.num_labels, bias=False,dtype=torch.float16) #bias is false as finetuned with and without and made no diff so saves memory\n",
        "        self.model.load_adapter(peft_model_id=r'seatond/identcontents_rank16_lr2.2e-05_target3_39steps_laplha32_batch1_gradacc4',adapter_name='ident_cont_adapter')\n",
        "        self.contclass_head = self.model.score\n",
        "        return None\n",
        "\n",
        "    def load_splitcausal_adapter(self):\n",
        "        self.model.lm_head = self.causal_head\n",
        "        self.model.load_adapter(peft_model_id='seatond/revamped_rank64_batch4',adapter_name='splitcausal_adapter') #should make the adapter paths etc dynamic\n",
        "\n",
        "    def load_extrcontcausal_adapter(self):\n",
        "        self.model.lm_head = self.causal_head\n",
        "        self.model.load_adapter(peft_model_id='seatond/revamped_rank64_batch4',adapter_name='extrcontcausal_adapter')\n",
        "\n",
        "    def change_adapter(self,desired): #desired should be classification,causal,base\n",
        "        if desired == 'fpclass':\n",
        "            self.model.lm_head = None #replaced LM_head with score which maps to our 2 classes\n",
        "            self.model.config.lm_head = None  #remove mpapping to vocab size\n",
        "            self.model.conditioner = 'classification' #added if statement in source code to direct to correct type of forward pass for classification.\n",
        "            self.model.score = self.fpclass_head #sets the 2 class head\n",
        "            self.model.enable_adapters()\n",
        "            self.model.set_adapter('ident_fistpage_adapter')\n",
        "        if desired == 'contclass':\n",
        "            self.model.lm_head = None #replaced LM_head with score which maps to our 2 classes\n",
        "            self.model.config.lm_head = None  #remove mpapping to vocab size\n",
        "            self.model.conditioner = 'classification' #added if statement in source code to direct to correct type of forward pass for classification.\n",
        "            self.model.score = self.contclass_head #sets the 2 class head\n",
        "            self.model.enable_adapters()\n",
        "            self.model.set_adapter('ident_cont_adapter')\n",
        "        if desired == 'splitcausal':\n",
        "            self.model.score = None\n",
        "            self.model.config.score = None\n",
        "            self.model.conditioner = 'causal'\n",
        "            self.model.lm_head = self.causal_head\n",
        "            self.model.enable_adapters()\n",
        "            self.model.set_adapter('splitcausal_adapter')\n",
        "        if desired == 'extrcontcausal':\n",
        "            self.model.score = None\n",
        "            self.model.config.score = None\n",
        "            self.model.conditioner = 'causal'\n",
        "            self.model.lm_head = self.causal_head\n",
        "            self.model.enable_adapters()\n",
        "            self.model.set_adapter('extrcontcausal_adapter')\n",
        "        if desired == 'base':\n",
        "            self.model.score = None\n",
        "            self.model.config.score = None\n",
        "            self.model.conditioner = 'causal'\n",
        "            self.model.lm_head = self.causal_head #looks similar to causal as base is a mistralforcausal LM\n",
        "            self.model.disable_adapters()\n",
        "\n",
        "\n",
        "    def get_contents(self):\n",
        "            self.change_adapter('contclass')\n",
        "            prompt = \"\"\"<s>[INST] @@@ Instructions:\n",
        "        It is your task to classify whether a string corresponds to the contents page of a pdf book.\n",
        "        A contents page includes chapter titles and page numbers.\n",
        "        Only reply with the words \"Yes\" or \"No\"\n",
        "        You must reply \"yes\" if the string is from the contents page, and \"no\" if it is not the contents page.\n",
        "\n",
        "        @@@ Question:\n",
        "        This is the string: ### \"\"\"\n",
        "            generation_config = GenerationConfig(\n",
        "                do_sample=True,\n",
        "                top_p=0.95, top_k=40,\n",
        "                temperature=0.7,\n",
        "                max_new_tokens=150,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "            for i in range(0,30):\n",
        "                input_ids = self.tokenizer(prompt+self.book.pages[i].extract_text()+' ### [/INST]',return_tensors='pt').input_ids.cuda()\n",
        "                input_ids = input_ids.to(\"cuda:0\") #making sure not on CPU as causes error\n",
        "                self.model = self.model.to(\"cuda:0\")\n",
        "                with torch.no_grad(): #no grad as dont want to change any weights just doing an inference\n",
        "                    predicted_class = torch.argmax(\n",
        "                        self.model(input_ids).logits\n",
        "                        ).item() # the inner bit of argmax just gets the logits, we then take max value to get predicted class\n",
        "                    #no need for softmax as only 2 classes.\n",
        "                if predicted_class == 1:\n",
        "                    self.contents_first = i\n",
        "                    break\n",
        "            for i in range(self.contents_first,30):\n",
        "                input_ids = self.tokenizer(prompt+self.book.pages[i].extract_text()+' ### [/INST]',return_tensors='pt').input_ids.cuda()\n",
        "                input_ids = input_ids.to(\"cuda:0\") #making sure not on CPU as causes error\n",
        "                self.model = self.model.to(\"cuda:0\")\n",
        "                with torch.no_grad(): #no grad as dont want to change any weights just doing an inference\n",
        "                    predicted_class = torch.argmax(\n",
        "                        self.model(input_ids).logits\n",
        "                        ).item()\n",
        "                if predicted_class == 0:\n",
        "                    self.contents_last = i-1\n",
        "                    break\n",
        "            #could make 2 things above a funciton to reduce repetition?\n",
        "            return 'found contents page .. hopefully'\n",
        "\n",
        "    def extract_contents(self):\n",
        "        self.change_adapter('extrcontcausal')\n",
        "        contents_pages = \"\" #if method too long can feed separate prompts to LLM for each page of contents page.\n",
        "        for i in range(self.contents_first,self.contents_last+1):\n",
        "            contents_pages += self.book.pages[i].extract_text()\n",
        "\n",
        "        prompt = \"\"\"<s>[INST] @@@ Instructions:\n",
        "It is your task to extract the chapters and corresponding page numbers from a string which was created from the contents page of a pdf book.\n",
        "You must return a list of the chapters and page numbers.\n",
        "Put each chapter and its page number on its own line, and separate chapters titles from page numbers with a \"---\".\n",
        "You will be penalised for separating chapters with anything that is not \"---\"\n",
        "For example the first 2 chapters of a contents page should be in the following format: \"chapter 1 title --- chapter 1 page number \\n chapter 2 title --- chapter 2 page number\"\n",
        "\n",
        "@@@ Question:\n",
        "string which was created from the contents page of a pdf book: ### \"\"\"\n",
        "\n",
        "        input_ids = self.tokenizer(prompt+book.pages[i].extract_text()+' ### [/INST]',return_tensors='pt').input_ids.cuda()\n",
        "        output = self.tokenizer.decode((self.model.generate(inputs=input_ids, temperature=0.07, do_sample=True, top_p=0.35, top_k=5, max_new_tokens=400000))[0])\n",
        "\n",
        "        #return output\n",
        "\n",
        "        contents_list = [i.split('---') for i in output.split('\\n')]\n",
        "        contents_df = pd.DataFrame(contents_list,columns=['chapter_titles','page_number'])\n",
        "\n",
        "        mask = contents_df['page_number'].notna()\n",
        "        contents_df = contents_df[mask]\n",
        "        contents_df['page_number'] = contents_df['page_number'].apply(lambda x: x.replace(' ',''))\n",
        "        mask = contents_df['page_number'].apply(lambda x: str(x).isnumeric())\n",
        "        contents_df = contents_df[mask]\n",
        "        contents_df.reset_index(inplace=True)\n",
        "        contents_df.drop(columns=['index'],inplace=True)\n",
        "        contents_df['page_number'] = contents_df['page_number'].apply(lambda x: int(x))\n",
        "        self.contents = contents_df\n",
        "\n",
        "        return contents_df\n",
        "\n",
        "    def find_first_page(self):\n",
        "        self.change_adapter('fpclass')\n",
        "        self.model = self.model.to(\"cuda:0\")\n",
        "        for i in range(self.contents_last+1,self.contents_last+30): #gonna loop through pages until find first page of chapter\n",
        "            #each iteration will do a forward pass through the model with adapters with page i book\n",
        "            prompt = f\"\"\"<s>[INST] This is a string from a page of a pdf book: ### {self.book.pages[i].extract_text()} ###\n",
        "            Is it true or false that this page belongs to a chapter called: ### {self.contents['chapter_titles'][0]} ###? [/INST]\"\"\"\n",
        "            input_ids = self.tokenizer(prompt,return_tensors='pt').input_ids.to(\"cuda:0\")\n",
        "            input_ids = input_ids.to(\"cuda:0\") #making sure not on CPU as causes error\n",
        "\n",
        "            with torch.no_grad(): #no grad as dont want to change any weights just doing an inference\n",
        "                predicted_class = torch.argmax(\n",
        "                    self.model(input_ids).logits\n",
        "                    ).item() # the inner bit of argmax just gets the logits, we then take max value to get predicted class\n",
        "                #no need for softmax as only 2 classes\n",
        "\n",
        "            if predicted_class == 1:\n",
        "                self.page_lag = i - (self.contents['page_number'][0]) #getting diff betweeen contents page of title and actual page\n",
        "                break\n",
        "            else:\n",
        "                None\n",
        "\n",
        "        self.contents['corrected_page_number'] = self.contents['page_number'].apply(lambda x: x + self.page_lag)\n",
        "\n",
        "        #could make 2 things above a funciton to reduce repetition?\n",
        "        return 'found contents page .. hopefully'\n",
        "\n",
        "###########################################retreival part done now\n",
        "\n",
        "    def split_chosen_chapter(self,chosen_chapter): #if results come out bad can do the thing where chop off end of page and put into next one so dont get crap endings.\n",
        "        self.change_adapter('splitcausal')\n",
        "        self.model = self.model.to(\"cuda:0\")\n",
        "        chosen_chapter_index = self.contents[self.contents['chapter_titles'] == chosen_chapter].index[0] #gets row of chosen chapter in contents df\n",
        "        responses = \"\"\n",
        "        for i in range(self.contents.iloc[chosen_chapter_index,2],self.contents.iloc[chosen_chapter_index,2]+2): #between the page numbers of chosen chapter and next chapter\n",
        "            prompt = f\"\"\"[INST]You must split text up into subsections and add informative titles for each subsection.\n",
        "Each subsection must be in paragraph form and all information should be included from the original text.\n",
        "You will be penalized for removing information from the original text.\n",
        "Mark each title you create by adding the symbols \"@@@\" before each title and placing the title on its own line.\n",
        "An example subsection format is \"@@@title \\n content\", where you should add the subsection title and content.\n",
        "This is the text:\n",
        "### {self.book.pages[i].extract_text()} ### [/INST]\n",
        "Output: \"\"\"\n",
        "\n",
        "            input_ids = self.tokenizer(prompt,return_tensors='pt').input_ids.to(\"cuda:0\")\n",
        "            input_ids = input_ids.to(\"cuda:0\")\n",
        "\n",
        "            generation_config = GenerationConfig(\n",
        "              do_sample=True,\n",
        "              top_p=0.95, top_k=40,\n",
        "              temperature=0.7,\n",
        "              max_new_tokens=150000,\n",
        "              eos_token_id=self.tokenizer.eos_token_id,\n",
        "              pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "\n",
        "            out = self.model.generate(inputs=input_ids, generation_config=generation_config)\n",
        "            decoded_output = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "            decoded_output = decoded_output[\n",
        "                decoded_output.find(\"[/INST]\")\n",
        "                + len(\"[/INST]\"):\n",
        "            ]\n",
        "\n",
        "            responses += decoded_output\n",
        "\n",
        "\n",
        "        responses = responses.split('@@@')[1:]\n",
        "        self.chapter_breakdown = pd.DataFrame({\n",
        "            'subtitle':list(map(lambda x: x[:x.find('\\n')],responses)),\n",
        "            'text':list(map(lambda x: x[x.find('\\n')+2:],responses))})\n",
        "\n",
        "##currently base_model and peft model are mixed up, choose a way which doesnt do memory in.\n",
        "#havent set pad tpoken, try with and without will i need to re finetune??\n",
        "# warning about pad token and also attnetnion mask, do we need it?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBqSTrvqAzVM"
      },
      "outputs": [],
      "source": [
        "#test classification score..\n",
        "\n",
        "def prep_data(test_data_path):\n",
        "    data = pd.read_json(test_data_path)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"content\": data.apply(\n",
        "                lambda x: f\"\"\"<s>[INST] This is a string from a page of a pdf book: ### {x['page']} ###\n",
        "                Is it true or false that this page belongs to a chapter called: ### {x['chapter_title']} ###? [/INST]\"\"\",\n",
        "                axis=1,\n",
        "            ),\n",
        "            'label': data['label']\n",
        "        }\n",
        "    )\n",
        "    #return data_aq\n",
        "    return df\n",
        "\n",
        "def inference(tokenizer,model,test_data):\n",
        "    ground = []\n",
        "    pred = []\n",
        "    model = model.to(\"cuda:0\")\n",
        "    for i in range(0,38):\n",
        "        print(i)\n",
        "        X_test_tokenized = tokenizer(test_data['content'][i], return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "        with torch.no_grad():\n",
        "            logits = model(X_test_tokenized).logits\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get the predicted class index\n",
        "        predicted_class = torch.argmax(probs).item()\n",
        "        predicted_class = 'yes' if predicted_class == 1 else 'No'\n",
        "\n",
        "        pred.append(predicted_class)\n",
        "        ground.append(test_data['label'][i])\n",
        "        print(i)\n",
        "    return pd.DataFrame({'ground':ground,'pred':pred})\n",
        "\n",
        "from auto_gptq import exllama_set_max_input_length\n",
        "obj.model = exllama_set_max_input_length(obj.model, max_input_length=23010)\n",
        "\n",
        "obj.set_adapter('classification')\n",
        "test_data_path = r\"../../Colab Notebooks/firstpage_testdata.json\"\n",
        "test_data = prep_data(test_data_path)\n",
        "results = inference(obj.tokenizer,obj.model,test_data)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCpsZTesPS9r",
        "outputId": "e1adddb8-2b15-423c-8828-8c5de139a8ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[10.1016, -0.9805]], device='cuda:0', dtype=torch.float16)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#logits test\n",
        "def prep_data(test_data_path):\n",
        "    data = pd.read_json(test_data_path)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"content\": data.apply(\n",
        "                lambda x: f\"\"\"<s>[INST] This is a string from a page of a pdf book: ### {x['page']} ###\n",
        "                Is it true or false that this page belongs to a chapter called: ### {x['chapter_title']} ###? [/INST]\"\"\",\n",
        "                axis=1,\n",
        "            ),\n",
        "            'label': data['label']\n",
        "        }\n",
        "    )\n",
        "    #return data_aq\n",
        "    return df\n",
        "def logits_test(obj,test_data,tokenizer,model):\n",
        "    model = model.to(\"cuda:0\")\n",
        "    input_ids = tokenizer(test_data['content'][20], return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "    with torch.no_grad(): #no grad as dont want to change any weights just doing an inference\n",
        "        log = obj.model(input_ids).logits\n",
        "    return log\n",
        "\n",
        "\n",
        "from auto_gptq import exllama_set_max_input_length\n",
        "obj.model = exllama_set_max_input_length(obj.model, max_input_length=23010)\n",
        "obj.set_adapter('classification')\n",
        "test_data_path = r\"../../Colab Notebooks/firstpage_testdata.json\"\n",
        "test_data = prep_data(test_data_path)\n",
        "logits_test(obj,test_data,obj.tokenizer,obj.model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test fp ident\n",
        "def prep_data(test_data_path):\n",
        "    df = pd.read_csv(test_data_path)\n",
        "    return df\n",
        "def inference(tokenizer,model3,test_data):\n",
        "    ground = []\n",
        "    pred = []\n",
        "    for i in range(0,len(test_data)):\n",
        "        print(i)\n",
        "        X_test_tokenized = tokenizer(test_data['x_test'][i], return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "        X_test_tokenized = X_test_tokenized.to(\"cuda:0\") #making sure not on CPU as causes error\n",
        "        model = model3.to(\"cuda:0\")\n",
        "        with torch.no_grad(): #no grad as dont want to change any weights just doing an inference\n",
        "            predicted_class = torch.argmax(\n",
        "                model(X_test_tokenized).logits\n",
        "                ).item()\n",
        "\n",
        "        pred.append(predicted_class)\n",
        "        ground.append(test_data['y_test'][i])\n",
        "        print(i)\n",
        "\n",
        "    # %%\n",
        "    return pd.DataFrame({'ground':ground,'pred':pred})\n",
        "\n",
        "\n",
        "test_data_path = r'../../contents_ident_and_extract/data/classification_test_data.csv'\n",
        "test_data = prep_data(test_data_path)\n",
        "results = inference(obj.tokenizer,obj.model,test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMn0-ssAQ9EZ",
        "outputId": "ca6bbbcc-4782-4a37-ddf6-c032f65d0908"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
            "  warnings.warn(warning_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "base_model_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "#book = PdfReader(r\"../../edexcel_a_level_physics_student_book_1.pdf\")\n",
        "book = PdfReader(r\"../../business book for testing.pdf\")\n",
        "\n",
        "obj = app(base_model_path,book)\n",
        "obj.get_base()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oprVMNL8kKCu"
      },
      "outputs": [],
      "source": [
        "mod,tok = obj.model,obj.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVLtoqJaXU3Z"
      },
      "outputs": [],
      "source": [
        "obj.load_fpclass_adapter()\n",
        "obj.load_contclass_adapter()\n",
        "obj.load_splitcausal_adapter()\n",
        "obj.load_extrcontcausal_adapter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2eT1aJW7jVyw",
        "outputId": "99e6206f-4a02-4283-db44-15aba798c5af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'found contents page .. hopefully'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_contents(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhwc3JkRoH7m",
        "outputId": "300f80f7-a81b-475a-c1ea-464edd76c1d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "obj.contents_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVOtE4BnsSPy"
      },
      "outputs": [],
      "source": [
        "#obj.model,obj.tokenizer = mod,tok\n",
        "obj.contents_first,obj.contents_last = 2,4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj5Vdt-ROjDL"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "hi = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IPgWOToKkVgA",
        "outputId": "9f1cb6ac-010e-4898-b85a-8858f9aebeab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " yes</s>\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'found contents page .. hopefully'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#obj.set_adapter('base')\n",
        "#obj.contents_first = 2\n",
        "obj.model.conditioner = 'causal'\n",
        "get_contents(obj)\n",
        "#print(obj.contents_first,obj.contents_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLL-1G-5are8"
      },
      "outputs": [],
      "source": [
        "def extract_contents(self):\n",
        "        self.set_adapter('base')\n",
        "        #contents_pages = \"\" #if method too long can feed separate prompts to LLM for each page of contents page.\n",
        "        #for i in range(3,4):\n",
        "            #contents_pages += self.book.pages[i].extract_text()\n",
        "            #contents_pages = pdf_text\n",
        "        prompt = \"\"\"<s>[INST] @@@ Instructions:\n",
        "It is your task to extract the chapters and corresponding page numbers from a string which was created from the contents page of a pdf book.\n",
        "You must return a list of the chapters and page numbers.\n",
        "Put each chapter and its page number on its own line, and separate chapters titles from page numbers with a \"---\".\n",
        "You will be penalised for separating chapters with anything that is not \"---\"\n",
        "For example the first 2 chapters of a contents page should be in the following format: \"chapter 1 title --- chapter 1 page number \\n chapter 2 title --- chapter 2 page number\"\n",
        "\n",
        "@@@ Question:\n",
        "string which was created from the contents page of a pdf book: ### \"\"\"\n",
        "\n",
        "        #input_ids = self.tokenizer(prompt+book.pages[i].extract_text()+' ### [/INST]',return_tensors='pt').input_ids.cuda()\n",
        "        input_ids = self.tokenizer(prompt+pdf_text+' ### [/INST]',return_tensors='pt').input_ids.cuda()\n",
        "        output = self.tokenizer.decode((self.model.generate(inputs=input_ids, temperature=0.07, do_sample=True, top_p=0.35, top_k=5, max_new_tokens=400000))[0])\n",
        "\n",
        "        return output\n",
        "\n",
        "        contents_list = [i.split('---') for i in output.split('\\n')]\n",
        "        contents_df = pd.DataFrame(contents_list,columns=['chapter_titles','page_number'])\n",
        "\n",
        "        mask = contents_df['page_number'].notna()\n",
        "        contents_df = contents_df[mask]\n",
        "        contents_df['page_number'] = contents_df['page_number'].apply(lambda x: x.replace(' ',''))\n",
        "        mask = contents_df['page_number'].apply(lambda x: str(x).isnumeric())\n",
        "        contents_df = contents_df[mask]\n",
        "        contents_df.reset_index(inplace=True)\n",
        "        contents_df.drop(columns=['index'],inplace=True)\n",
        "        contents_df['page_number'] = contents_df['page_number'].apply(lambda x: int(x))\n",
        "        self.contents = contents_df\n",
        "\n",
        "        return contents_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03VAjh3IdKVS",
        "outputId": "addf94e2-41d9-4e7e-de65-d969a1e4ed71"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/app/code'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UooXjJqwchb7",
        "outputId": "524c0f4b-cf60-4872-9fc9-5845aa235a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iv\n",
            "COURSE STRUCTURE\n",
            "MEETING \n",
            "CUSTOMER NEEDS 2\n",
            "ENTREPRENEURS \n",
            "AND LEADERS \n",
            "140\n",
            "MARKETING MIX \n",
            "AND STRATEGY \n",
            "56\n",
            "1.  THE MARKET  \n",
            "3\n",
            "2. MARKET RESEARCH  \n",
            "11\n",
            " 3.  MARKET POSITIONING  \n",
            "20\n",
            "19.  ROLE OF AN  \n",
            "ENTREPRENEUR \n",
            "141\n",
            "20.  ENTREPRENEURIAL \n",
            "MOTIVES AND \n",
            "CHARACTERISTICS \n",
            "149\n",
            "21. BUSINESS OBJECTIVES 155\n",
            "22.  BUSINESS CHOICES \n",
            "160\n",
            " 9.  MARKETING OBJECTIVES  \n",
            "AND STRATEGY \n",
            "57\n",
            "10.  PRODUCT/SERVICE  \n",
            "DESIGN \n",
            "68\n",
            "11.  PROMOTION AND  \n",
            "BRANDING  \n",
            "74\n",
            "12. PRICING STRATEGIES \n",
            "83\n",
            "13.  DISTRIBUTION \n",
            " \n",
            "89\n",
            "THE MARKET \n",
            "30\n",
            "MANAGING  \n",
            "PEOPLE  \n",
            "95\n",
            "4.  DEMAND \n",
            " \n",
            "31\n",
            "5.  SUPPLY \n",
            "36\n",
            "6.  MARKETS \n",
            "41\n",
            "7.  PRICE ELASTICITY OF  \n",
            "DEMAND (PED) \n",
            "46\n",
            "8.  INCOME ELASTICITY OF  \n",
            "DEMAND (YED) \n",
            "52\n",
            "14.  APPROACHES TO  \n",
            "STAFFING \n",
            "96\n",
            "15.  RECRUITMENT,  \n",
            "SELECTION AND  \n",
            "TRAINING  \n",
            "104\n",
            "16.  ORGANISATIONAL  \n",
            "DESIGN \n",
            "113\n",
            "17.  MOTIVATION IN THEORY \n",
            "AND PRACTICE \n",
            " \n",
            " 121\n",
            "18. LEADERSHIP \n",
            " \n",
            "132\n",
            "UNIT 1: MARKETING AND PEOPLE\n",
            "SAMPLE\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def read_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    text = ''\n",
        "\n",
        "    for page_num in range(3,4):\n",
        "        page = doc[page_num]\n",
        "        text += page.get_text()\n",
        "\n",
        "    doc.close()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "pdf_path = r\"../../business book for testing.pdf\"\n",
        "pdf_text = read_pdf(pdf_path)\n",
        "print(pdf_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpjmHaGHb245",
        "outputId": "19408a59-901c-4143-c06c-d0aaf7acc318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iv COURSE STRUCTURE\n",
            "MEETING \n",
            "CUSTOMER NEEDS  2ENTREPRENEURS AND LEADERS\n",
            " 140MARKETING MIX AND STRATEG \n",
            "Y 56\n",
            "1.  THE MARKET  3\n",
            "2. MARKET RESEARCH  11\n",
            " 3.  MARKET POSITIONING  2019.  ROLE OF AN  \n",
            "ENTREPRENEUR  141\n",
            "20.  ENTREPRENEURIAL MOTIVES AND CHARA\n",
            "CTERISTICS  149\n",
            "21. BUSINESS OBJECTIVES  155\n",
            "22.  BUSINESS CHOICES  160 9.  MARKETING OBJECTIVES  AND STRATEG \n",
            "Y 57\n",
            "10.  PRODUCT/SERVICE  \n",
            "DESIGN  68\n",
            "11.  PROMOTION AND  BRANDING\n",
            "  74\n",
            "12. PRICING STRATEGIES  83\n",
            "13.  DISTRIBUTION   89\n",
            "THE MARKET  30 MANAGING  \n",
            "PEOPLE   95\n",
            "4.  DEMAND   31\n",
            "5.  SUPPL Y  36\n",
            "6.  MARKETS  41\n",
            "7.  PRICE ELASTICITY OF  \n",
            "DEMAND (PED)  46\n",
            "8.  INCOME ELASTICITY OF  DEMAND (YED)\n",
            " 5214.  APPROACHES TO  S \n",
            "TAFFING  96\n",
            "15.  RECRUITMENT,  \n",
            "SELECTION AND  TRAINING\n",
            "  104\n",
            "16.  ORGANISATIONAL  \n",
            "DESIGN  113\n",
            "17.  MOTIVATION IN THEOR\n",
            "Y \n",
            "AND PRACTICE    121\n",
            "18. LEADERSHIP   132UNIT 1: MARKETING AND PEOPLE\n",
            "M00_Business_FM_39170.indd   4 5/9/18   7:38 PM\n",
            "SAMPLE\n"
          ]
        }
      ],
      "source": [
        "print(obj.book.pages[3].extract_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87D6UyS0rkw7",
        "outputId": "83601feb-41cb-46bc-8a04-5a17cba1dee6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "hi = extract_contents(obj)\n",
        "#obj.extract_contents()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHh_By_te2hy",
        "outputId": "d40495ad-e5b4-4c57-ca49-202eba96df5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s><s> [INST] @@@ Instructions:\n",
            "It is your task to extract the chapters and corresponding page numbers from a string which was created from the contents page of a pdf book.\n",
            "You must return a list of the chapters and page numbers.\n",
            "Put each chapter and its page number on its own line, and separate chapters titles from page numbers with a \"---\".\n",
            "You will be penalised for separating chapters with anything that is not \"---\"\n",
            "For example the first 2 chapters of a contents page should be in the following format: \"chapter 1 title --- chapter 1 page number \n",
            " chapter 2 title --- chapter 2 page number\"\n",
            "\n",
            "@@@ Question:\n",
            "string which was created from the contents page of a pdf book: ### iv\n",
            "COURSE STRUCTURE\n",
            "MEETING \n",
            "CUSTOMER NEEDS 2\n",
            "ENTREPRENEURS \n",
            "AND LEADERS \n",
            "140\n",
            "MARKETING MIX \n",
            "AND STRATEGY \n",
            "56\n",
            "1.  THE MARKET  \n",
            "3\n",
            "2. MARKET RESEARCH  \n",
            "11\n",
            " 3.  MARKET POSITIONING  \n",
            "20\n",
            "19.  ROLE OF AN  \n",
            "ENTREPRENEUR \n",
            "141\n",
            "20.  ENTREPRENEURIAL \n",
            "MOTIVES AND \n",
            "CHARACTERISTICS \n",
            "149\n",
            "21. BUSINESS OBJECTIVES 155\n",
            "22.  BUSINESS CHOICES \n",
            "160\n",
            " 9.  MARKETING OBJECTIVES  \n",
            "AND STRATEGY \n",
            "57\n",
            "10.  PRODUCT/SERVICE  \n",
            "DESIGN \n",
            "68\n",
            "11.  PROMOTION AND  \n",
            "BRANDING  \n",
            "74\n",
            "12. PRICING STRATEGIES \n",
            "83\n",
            "13.  DISTRIBUTION \n",
            " \n",
            "89\n",
            "THE MARKET \n",
            "30\n",
            "MANAGING  \n",
            "PEOPLE  \n",
            "95\n",
            "4.  DEMAND \n",
            " \n",
            "31\n",
            "5.  SUPPLY \n",
            "36\n",
            "6.  MARKETS \n",
            "41\n",
            "7.  PRICE ELASTICITY OF  \n",
            "DEMAND (PED) \n",
            "46\n",
            "8.  INCOME ELASTICITY OF  \n",
            "DEMAND (YED) \n",
            "52\n",
            "14.  APPROACHES TO  \n",
            "STAFFING \n",
            "96\n",
            "15.  RECRUITMENT,  \n",
            "SELECTION AND  \n",
            "TRAINING  \n",
            "104\n",
            "16.  ORGANISATIONAL  \n",
            "DESIGN \n",
            "113\n",
            "17.  MOTIVATION IN THEORY \n",
            "AND PRACTICE \n",
            " \n",
            " 121\n",
            "18. LEADERSHIP \n",
            " \n",
            "132\n",
            "UNIT 1: MARKETING AND PEOPLE\n",
            "SAMPLE\n",
            " ### [/INST] 1. COURSE STRUCTURE --- iv\n",
            "2. MEETING CUSTOMER NEEDS --- 140\n",
            "3. MARKETING MIX AND STRATEGY --- 56\n",
            "4. The Market --- 3 --- \"---\"\n",
            "5. Market Research --- 11\n",
            "6. Market Positioning --- 20\n",
            "7. Role of an Entrepreneur --- 141\n",
            "8. Entrepreneurial Motives and Characteristics --- 149\n",
            "9. Business Objectives --- 155\n",
            "10. Business Choices --- 160\n",
            "11. Marketing Objectives and Strategy --- 57\n",
            "12. Product/Service Design --- 68\n",
            "13. Promotion and Branding --- 74\n",
            "14. Pricing Strategies --- 83\n",
            "15. Distribution --- 89\n",
            "16. Managing People --- 95\n",
            "17. Demand --- 31\n",
            "18. Supply --- 36\n",
            "19. Markets --- 41\n",
            "20. Price Elasticity of Demand (PED) --- 46\n",
            "21. Income Elasticity of Demand (YED) --- 52\n",
            "22. Approaches to Staffing --- 96\n",
            "23. Recruitment, Selection and Training --- 104\n",
            "24. Organisational Design --- 113\n",
            "25. Motivation in Theory and Practice --- 121\n",
            "26. Leadership --- 132\n",
            "27. Unit 1: Marketing and People --- 133\n",
            "\n",
            "\"Sample\" is not a chapter title, so it is not included in the output.</s>\n"
          ]
        }
      ],
      "source": [
        "print(hi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9-qrxYge4LL",
        "outputId": "6234f593-2a62-4aa8-ef9b-cd64f5946b3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Marketing and People --- 5/9/18 7:38 PM --- 4\n",
            "2. The Market --- 14\n",
            "3. Market Research --- 11\n",
            "---\n",
            "4. Market Positioning --- 2019.9\n",
            "5. Role of an Entrepreneur --- 141\n",
            "---\n",
            "6. Entrepreneurial Motives and Characteristics --- 149\n",
            "7. Business Objectives --- 155\n",
            "---\n",
            "8. Business Choices --- 160\n",
            "9. Marketing Objectives and Strategy --- 57.9\n",
            "---\n",
            "10. Product/Service Design --- 68\n",
            "11. Promotion and Branding --- 74\n",
            "---\n",
            "12. Pricing Strategies --- 83\n",
            "13. Distribution --- 89\n",
            "---\n",
            "14. Managing People --- 95\n",
            "15. Demand --- 31\n",
            "---\n",
            "16. Supply --- 36\n",
            "---\n",
            "17. Markets --- 41\n",
            "---\n",
            "18. Price Elasticity of Demand (PED) --- 46\n",
            "---\n",
            "19. Income Elasticity of Demand (YED) --- 52\n",
            "---\n",
            "20. Approaches to Staffing --- 96\n",
            "---\n",
            "21. Recruitment, Selection and Training --- 104\n",
            "---\n",
            "22. Organizational Design --- 113\n",
            "---\n",
            "23. Motivation in Theory and Practice --- 121\n",
            "---\n",
            "24. Leadership --- 13\n"
          ]
        }
      ],
      "source": [
        "print(\"1. Marketing and People --- 5/9/18 7:38 PM --- 4\\n2. The Market --- 14\\n3. Market Research --- 11\\n---\\n4. Market Positioning --- 2019.9\\n5. Role of an Entrepreneur --- 141\\n---\\n6. Entrepreneurial Motives and Characteristics --- 149\\n7. Business Objectives --- 155\\n---\\n8. Business Choices --- 160\\n9. Marketing Objectives and Strategy --- 57.9\\n---\\n10. Product/Service Design --- 68\\n11. Promotion and Branding --- 74\\n---\\n12. Pricing Strategies --- 83\\n13. Distribution --- 89\\n---\\n14. Managing People --- 95\\n15. Demand --- 31\\n---\\n16. Supply --- 36\\n---\\n17. Markets --- 41\\n---\\n18. Price Elasticity of Demand (PED) --- 46\\n---\\n19. Income Elasticity of Demand (YED) --- 52\\n---\\n20. Approaches to Staffing --- 96\\n---\\n21. Recruitment, Selection and Training --- 104\\n---\\n22. Organizational Design --- 113\\n---\\n23. Motivation in Theory and Practice --- 121\\n---\\n24. Leadership --- 13\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J9xGgAWz8kKk",
        "outputId": "2aa71683-38d5-408a-8220-2445c191dd13"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'found contents page .. hopefully'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "obj.find_first_page()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHOqPRt3Afw-",
        "outputId": "dfa91f28-755c-4d40-92d3-682b3a86be40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "obj.page_lag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9No7NZn0s-iy"
      },
      "outputs": [],
      "source": [
        "def split_chosen_chapter(self,chosen_chapter_index): #if results come out bad can do the thing where chop off end of page and put into next one so dont get crap endings.\n",
        "        self.set_adapter('causal')\n",
        "        self.model = self.model.to(\"cuda:0\")\n",
        "        responses = \"\"\n",
        "        #for i in range(self.contents.iloc[chosen_chapter_index,2],self.contents.iloc[chosen_chapter_index,2]+2): #between the page numbers of chosen chapter and next chapter\n",
        "        for i in range(53,54):\n",
        "            prompt = f\"\"\"<s>[INST]You must split text up into subsections and add informative titles for each subsection.\n",
        "Each subsection must be in paragraph form and all information should be included from the original text.\n",
        "You will be penalized for removing information from the original text.\n",
        "Mark each title you create by adding the symbols \"@@@\" before each title and placing the title on its own line.\n",
        "An example subsection format is \"@@@title \\n content\", where you should add the subsection title and content.\n",
        "This is the text:\n",
        "### {self.book.pages[i].extract_text()} ### [/INST]\n",
        "Output: \"\"\"\n",
        "\n",
        "            input_ids = self.tokenizer(prompt,return_tensors='pt').input_ids.to(\"cuda:0\")\n",
        "            input_ids = input_ids.to(\"cuda:0\")\n",
        "\n",
        "            generation_config = GenerationConfig(\n",
        "              do_sample=True,\n",
        "              top_p=0.95, top_k=40,\n",
        "              temperature=0.7,\n",
        "              max_new_tokens=1500,\n",
        "              eos_token_id=self.tokenizer.eos_token_id,\n",
        "              pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "\n",
        "            out = self.model.generate(inputs=input_ids, generation_config=generation_config)\n",
        "            decoded_output = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "            decoded_output = decoded_output[\n",
        "                decoded_output.find(\"[/INST]\")\n",
        "                + len(\"[/INST]\"):\n",
        "            ]\n",
        "\n",
        "            responses += decoded_output\n",
        "        return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "inb4218qCVnO",
        "outputId": "ea77c4f4-44d8-4b7a-aff1-915dabbf615e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nOutput: \\n\\n@@@Prior Knowledge for Understanding Momentum (44) \\n\\nTo fully comprehend the concept of momentum as presented in this chapter, you should:\\n\\n/uni279C have a solid grasp of the principles of rectilinear motion (Chapter 3).\\n/uni279C understand Newton's second and third laws of motion.\\n/uni279C be familiar with the difference between scalar and vector quantities.\\n/uni279C comprehend the meaning of mass.\\n/uni279C know Newton's equation of motion: F = ma.\\n\\nBefore proceeding, test your knowledge with the following tasks:\\n\\n1. Arrange the following physical quantities into scalar or vector quantities: acceleration, displacement, distance, mass, speed, time, and velocity.\\n2. Explain the difference between mass and weight.\\n3. Calculate the resultant force acting on a 2.0 kg mass falling through a liquid if the resistive force is 12 N.\\n4. What is the resultant force when forces of 3.0 N and 4.0 N act at right angles at a point?\\n5. Calculate the resultant force acting on a body of mass 5.6 kg that accelerates uniformly from rest to 14 m/s in 12 s.\\n6. Calculate the acceleration of a trailer of mass 1.2 tonnes if the pull of the car is 800 N and the resistive forces on the trailer total 200 N.\\n\\n@@@Momentum: Definition and Formula \\n\\nMomentum is a fundamental concept in classical mechanics that measures the amount of motion an object possesses. It is defined as the product of an object's mass and velocity:\\n\\nmomentum = mass × velocity\\np = mv\\n\\n@@@Momentum in Action: Pushing Supermarket Trolleys \\n\\nWhen you push a loaded supermarket trolley to link with an empty stationary one, the outcome depends on the speed and force with which you launch the loaded trolley and its mass. The product of a body's mass and velocity is referred to as the body's momentum or linear momentum, p. Understanding momentum and its conservation is essential for explaining how animals and vehicles move.\\n\\nIn this chapter, you will develop your understanding of Newtonian mechanics, focusing on collisions, recoils, and impulsive forces.\\n\\n@@@Linear Momentum \\n\\nWhen you push a loaded supermarket trolley to link with an empty stationary one, the outcome depends on the speed with which you launch the loaded trolley and the mass of the loaded trolley. The product of a body's mass and velocity is useful in analyzing such collisions: this product is called the body's momentum or linear momentum, p.\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chapter_num = 0\n",
        "test = split_chosen_chapter(obj,chapter_num) #so assuming here user has been presented with chapters and then selected the chapter num and then convert this to index by minus 1\n",
        "test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_nbkCE7dt8y"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMuT5ZilaRQy"
      },
      "outputs": [],
      "source": [
        "retrieve = retriever(book,model,tokenizer)\n",
        "#retrieve.get_contents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzOyhDKfDc7B",
        "outputId": "0625a0f9-0597-46e2-ad96-54ca802ed655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " chapter 1: introduction --- iv\n",
            "chapter 2: get the most from this book --- vi\n",
            "chapter 3: introduction ---\n",
            "chapter 1: quantities and units --- 1\n",
            "chapter 2: practical skills --- 10\n",
            "\n",
            "chapter 3: mechanics\n",
            "chapter 3.1: rectilinear motion --- 25\n",
            "chapter 3.2: momentum --- 44\n",
            "chapter 3.3: forces --- 55\n",
            "chapter 3.4: work, energy and power --- 73\n",
            "\n",
            "chapter 4: electric circuits\n",
            "chapter 4.1: charge and current --- 90\n",
            "chapter 4.2: potential difference, electromotive force and power --- 102\n",
            "chapter 4.3: current–potential difference relationships --- 116\n",
            "chapter 4.4: resistance and resistivity --- 128\n",
            "chapter 4.5: internal resistance, series and parallel circuits, and the potential divider --- 145\n",
            "\n",
            "chapter 5: materials\n",
            "chapter 5.1: fluids --- 173\n",
            "chapter 5.2: solid materials --- 188\n",
            "\n",
            "chapter 6: waves and the particle behaviour of light\n",
            "chapter 6.1: nature of waves --- 210\n",
            "chapter 6.2: transmission and reflection of waves --- 221\n",
            "chapter 6.3: superposition of waves --- 247\n",
            "chapter 6.4: particle nature of light --- 274\n",
            "\n",
            "chapter 7: conclusion ---\n",
            "chapter 8: maths in physics --- 296\n",
            "chapter 9: preparing for the exams --- 310\n",
            "chapter 10: index --- 323\n",
            "chapter 11: free online resources --- 327</s>\n"
          ]
        }
      ],
      "source": [
        "retrieve.contents_first=4\n",
        "retrieve.contents_last =4\n",
        "print(retrieve.extract_contents())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5hdLdJ4VN2-",
        "outputId": "30178400-2af4-4d91-b488-48841115bf08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieve.contents_first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq0VxMr9VY98",
        "outputId": "28b14b01-a4c1-41c3-adce-c02ab448b45f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieve.contents_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3FZP2jY3e3i"
      },
      "outputs": [],
      "source": [
        "from auto_gptq import exllama_set_max_input_length\n",
        "model = exllama_set_max_input_length(model, max_input_length=6000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "INvhgcE018s1",
        "outputId": "679598f0-d6af-4c1e-f988-87e59baca0b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' yes</s>'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "title = 'Quantities and units'\n",
        "string = '1 1.1 Physical quantities, base and derived units \\n1.1 Physical quantities, base \\nand derived units\\nAn elderly physicist was asked how much he had in the bank.\\n‘How much of what?’ he responded.\\n‘Money, of course!’\\n‘Fifteen million, three hundred thousand, one hundred and four,’ he replied.\\nThe physicist was not a rich man. He had quoted his balance in Turkish \\nLira which, at that time, had an exchange rate of 2.6 million to the pound \\n(1.4 million to the American dollar).\\nThe story has relevance to measurements in physics. It is meaningless to state that \\nthe size of a wire is 10; we must state the quantity that is measured (in this case,\\xa0the \\nlength of the wire) and the unit (such as cm). In this chapter, and throughout this \\nbook, you will identify and use a number of base quantities  (and their units) \\nthat are fundamental to all physical measurements. You will develop and use \\nderived units for quantities for a wide range of physical properties.Prior knowledge\\nIn this chapter you will need to:\\n/uni279C use common measures and simple compound measures such as speed \\n/uni279C substitute values into formulae and equations using appropriate units and \\nrearrange equations in order to change the subject. \\nThe key facts that will be useful are:\\n/uni279C mass, length and time are examples of measurable physical quantities\\n/uni279C kilogram (kg), metre (m) and second (s) are units of mass, length and time\\n/uni279C speed is the distance covered per unit time and is measured in metres per \\nsecond (m s–1)\\n/uni279C vector quantities have both size and direction. 1 Quantities and units\\nTest yourself on prior knowledge\\n1 A man walks 1.6 km in 20 minutes. Calculate his average speed.\\n2 A sprinter runs 200 m at an average speed of 8.0 m s –1. Calculate the \\ntime taken for her to complete the distance.\\n3 Acceleration can be calculated by dividing a change in speed by the \\ntime taken. State the unit of acceleration.\\n4 Speed is a scalar quantity and velocity is a vector. Explain the \\ndifference between the two.\\nTip\\nMany students lose marks in \\nexaminations by failing to include the \\nunit of a derived quantity! Always show \\nthe unit for all calculated quantities.\\n807527_C01_Edexcel_Physics_001-009.indd   1 24/02/2015   14:06www.ebook3000.com\\n'\n",
        "prompt = f\"\"\"<s>[INST] @@@ Instructions:\n",
        "You are an assisstant who must classify whether a string from a page of a pdf book corresponds to the first page of a given chapter in that book.\n",
        "You will be given the string and also given the chapter title.\n",
        "The first page of a chapter usually contains the name of the chapter towards the start of the string.\n",
        "The first word of you answer must be \"Yes\" or \"No\"\n",
        "You must reply \"yes\" if the string is the first page of the given chapter, and \"no\" if it is not the first page of the given chapter.\n",
        "\n",
        "@@@ Example:\n",
        "User: The given chapter title is ### The Chemistry Of Life ### and the string is: ### 'CHEMISTRY FOR BIOLOGISTS 61A.1 THE CHEMISTRY OF LIFE\\nTHE CHEMISTRY OF WATER\\nAll reactions in living cells take place in water. Without water, \\nsubstances could not move around the body. Water is one of  the reactants in the process of  photosynthesis, on which almost all life depends (see fig E). Understanding the properties of  water will help you understand many key systems in living organisms. \\nWater is also a major habitat – it supports more life than any other \\npart of  the planet.\\n▲ fig E  W ater is vital for life on Earth in many different ways – in a desert, \\nthe smallest amount of water allows plants to grow.\\nThe simple chemical formula of  water is H2O. This tells us that \\ntwo atoms of  hydrogen are joined to one atom of  oxygen to make \\nup each water molecule. However, because the electrons are held closer to the oxygen atom than to the hydrogen atoms, water is a polar molecule (see fig F).\\n104.5°Oδ2\\nHδ1Hδ1\\n▲ fig F  A model of a w ater molecule showing dipoles.\\nOne major effect of  this polarity is that water molecules form hydrogen  bonds. The slightly negative oxygen atom of  one water \\nmolecule will attract the slightly positive hydrogen atoms of  other water molecules in a weak electrostatic attraction called a hydrogen \\nbond. Each individual hydrogen bond is weak but there are many of  them so the molecules of  water ‘stick together’ more than you might expect (see fig G). Water has relatively high melting and boiling points compared with other substances that have molecules of  a similar size because it takes a lot of  energy to break all the hydrogen bonds that hold the molecules together. Hydrogen bonds are important in protein structure (see Sections 1A.5 and 2B.1) \\nand in the structure and functioning of  DNA (see Section 2B.3).Oδ2\\nOδ2\\nHδ1Hδ1Hδ1Hδ1\\nHδ1Oδ2Oδ2\\nOδ2Hδ1\\nHδ1\\nHδ1Hδ1Hδ1\\n▲ fig G  Hydr ogen bonding in water molecules, based on attraction \\nbetween positive and negative dipoles.\\nTHE IMPORTANCE OF WATER\\nThe properties of  water make it very important in biological \\nsystems for many reasons.\\n •W ater is a polar solvent. Because it is a polar molecule, many ionic \\nsubstances like sodium chloride will dissolve in it (see fig H).  \\nMany covalently bonded substances are also polar and will dissolve in water, but often do not dissolve in other covalently bonded solvents such as ethanol. Water also carries other substances, such as starch. As a result, most of  the chemical reactions within cells occur in water (in aqueous solution).\\nsodium and chloride ionsin solution in water\\nsalt and water mixed\\nsodium chloride\\nNaClionic bond sodium ionchlorideionδ1 chargeson hydrogenin water areattracted tonegativechloride ion\\nδ2\\n charges\\non oxygenin water ar\\ne\\nattracted tothe positivesodium ionH\\nH\\nHHH\\nH HO\\nOO\\nCl2\\nCl2\\nCl2Cl2\\nCl2\\nNa1OH\\nCl2\\nNa1Na1\\nCl2Cl2\\nCl2\\nCl2Cl2\\nNa1Na1Na1 Na1\\nNa1Na1Na1\\n▲ fig H  A model of sodium chloride dissolving in water as a result of the \\ninteractions between the charges on sodium and chloride ions and the dipoles of the water molecules.\\nUncorrected proof, all content subject to change at publisher discretion. Not for resale, circulation or distribution in whole or in part. ©Pearson 2018' ###\n",
        "Assisstant: \"Yes\"\n",
        "\n",
        "@@@ Example:\n",
        "User: The given chapter title is ### Preparing For Your Exams ### and the string is: ### 'xASSESSMENT OVERVIEW\\nPAPER / UNIT 1PERCENTAGE \\nOF IASPERCENTAGE OF IALMARK TIME AVAILABILITY\\nMOLECULES, DIET, TRANSPORT AND \\nHEALTH \\nWritten examination\\nPaper code \\nWBI11/01\\nExternally set and marked by \\nPearson Edexcel\\nSingle tier of entry40% 20% 80 1 hour  \\n30 minutesJanuary, June and October\\nFirst assessment : January 2019\\nPAPER / UNIT 2PERCENTAGE \\nOF IASPERCENTAGE OF IALMARK TIME AVAILABILITY\\nCELLS, DEVELOPMENT, BIODIVERSITY \\nAND CONSERVATION\\nWritten examination\\nPaper code \\nWBI12/01\\nExternally set and marked by \\nPearson Edexcel\\nSingle tier of entry40% 20% 80 1 hour  \\n30 minutesJanuary, June and October\\nFirst assessment : June 2019\\nPAPER / UNIT 3PERCENTAGE \\nOF IASPERCENTAGE OF IALMARK TIME AVAILABILITY\\nPRACTICAL SKILLS IN BIOLOGY 1   \\nWritten examination\\nPaper code \\nWBI13/01\\nExternally set and marked by \\nPearson Edexcel\\nSingle tier of entry20% 10% 50 1 hour  \\n20 minutesJanuary, June and October\\nFirst assessment : June 2019ASSESSMENT OVERVIEW\\nThe following tables give an overview of the assessment for Pearson Edexcel International Advanced Subsidiary course \\nin Biology. You should study this information closely to help ensure that you are fully prepared for this course and know exactly what to expect in each part of the examination. More information about this qualification, and about the question types in the different papers, can be found on page 302 of this book.\\nUncorrected proof, all content subject to change at publisher discretion. Not for resale, circulation or distribution in whole or in part. ©Pearson 2018' ###\n",
        "Assisstant: \"No\"\n",
        "\n",
        "@@@ Question:\n",
        "User: The given chapter title: ### {title} ### This is the string: ### {string} ###\n",
        "[/INST]\"\"\"\n",
        "\n",
        "\n",
        "input_ids = tokenizer(prompt,return_tensors='pt').input_ids.cuda()\n",
        "output = tokenizer.decode((model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=5))[0])\n",
        "output[output.find('[/INST]')+len('[/INST]'):].lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pioxF_Cq5tQw"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
